{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "GKPL0iARvKiY"
   },
   "source": [
    "#  Lab 07 - Knowledge Tracing\n",
    "\n",
    "## Introduction\n",
    "\n",
    "During the last lectures and lab session, you have dealt into one notable application of machine learning in education, namely knowledge tracing. Machine-learning models optimized for this task aim to understand how well a student is learning a portfolio of skills. Monitoring this knowledge by means of automated models allows to personalize online learning platforms, focusing the assessment on skills the student is weak in and accelerating learning of certain skills.\n",
    "\n",
    "You are asked to work on the ASSISTment data set presented last week and to complete the following tasks:\n",
    "\n",
    "- Compare three knowledge tracing models (BKT, AFM, PFA) in terms of AUC and RMSE.\n",
    "- Generate and discuss the learning curves for a BKT model on a specific set of skills. \n",
    "\n",
    "You can use [pyBKT](https://github.com/CAHLR/pyBKT) and [pyAFM](https://github.com/cmaclell/pyAFM/) throughout this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BY812cUCvKie"
   },
   "outputs": [],
   "source": [
    "# Principal package imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "# Scikit-learn package imports\n",
    "from sklearn import feature_extraction, model_selection, metrics\n",
    "\n",
    "# PyBKT package imports\n",
    "from pyBKT.models import Model\n",
    "\n",
    "# PyAFM package imports\n",
    "from pyafm.custom_logistic import CustomLogistic\n",
    "\n",
    "### YOUR ADDITIONAL IMPORT STATEMENTS BELOW (please, do not make any imports elsewhere in the notebook) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfGO_wGvvKif"
   },
   "source": [
    "## The Data Set\n",
    "---\n",
    "\n",
    "ASSISTments is a free tool for assigning and assessing math problems and homework. Teachers can select and assign problem sets. Once they get an assignment, students can complete it at their own pace and with the help of hints, multiple chances, and immediate feedback. Teachers get instant results broken down by individual student or for the whole class. More information on the platform can be found [here](https://www.commonsense.org/education/website/assistments). \n",
    "\n",
    "In this homework, we will play with a simplified version of a dataset collected from the ASSISTments tool, saved in a CSV file with the following columns:  \n",
    "\n",
    "\n",
    "| Name                   | Description                         |\n",
    "| ---------------------- | ------------------------------------------------------------ |\n",
    "| user_id | The ID of the student who is solving the problem.  | |\n",
    "| order_id | The temporal ID (timestamp) associated with the student's answer to the problem.  | |\n",
    "| problem_id | The ID of the problem.  | |\n",
    "| skill_name | The name of the skill associated with the problem. | |\n",
    "| correct | The student's performance on the problem: 1 if the problem's answer is correct at the first attempt, 0 otherwise. \n",
    "| prior_success | The number of prior problems on that skill the student correctly answered at the first attempt. \n",
    "| prior_failure | The number of prior problems on that skill the student wrongly answered at the first attempt.  | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypX6jsWjvKig"
   },
   "source": [
    "Load the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LiKya7oLvKig"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./../../data/\"\n",
    "data = pd.read_csv(DATA_DIR + 'as_hw_cmp.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the total number of interactions, the number of unique students, and the number of unique skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are those skills?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnyKfbhyvKik"
   },
   "source": [
    "<a id=\"section1\"></a>\n",
    "## 1  Knowledge Tracing: Model Performance Comparison \n",
    "----\n",
    "\n",
    "In this section, we ask you to evaluate (i) a Bayesian Knowledge Tracing (BKT) model, (ii) an Additive Factor Model (AFM), and (iii) a Performance Factor Analysis (PFA) model on the skills 'Circle Graph', 'Venn Diagram', and 'Mode', by performing a user-stratified 10-fold cross validation and monitoring the Root Mean Squared Error (RMSE) and the Area Under the ROC Curve (AUC) as performance metrics. Then, we ask you to visually report the RMSE and AUC scores achieved by the three student's models in the user-stratified 10-fold cross validation, in such a way that the models' performance can be easily and appropriately compared against each other.\n",
    "\n",
    "For your convenience, you will be guided in completing this section through six main tasks:\n",
    "- Task 1.1: Group k-fold initialization.\n",
    "- Task 1.2: BKT evaluation.\n",
    "- Task 1.3: AFM evaluation.\n",
    "- Task 1.4: PFA evaluation.\n",
    "- Task 1.5: Performance metrics plotting.\n",
    "- Task 1.6: Performance metrics discussion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6sl6eJevKil"
   },
   "source": [
    "<a id=\"section1.1\"></a>\n",
    "### Task 1.1\n",
    "\n",
    "Given that the main objective of this homework section is to evaluate three student's knowledge tracing models under a `user-stratified 10-fold cross validation`, in this task, we ask you to complete the body of a function named `create_iterator`. This function should create an iterator object able to split student's interactions included in `data` in `10 folds` such that the same student does not appear in multiple folds. To do so, you can appropriately initialize a scikit-learn's GroupKFold iterator with non-overlapping groups and returning the iterator, i.e., `model_selection.GroupKFold(...).split(...)`. \n",
    "\n",
    "For convenience, we present you an illustrative example assuming that (i) you have four data samples and that (ii) the first two data samples belong to group 0 and the last two data samples belong to group 2. The data samples associated with a group should not appear in multiple folds or, in other words, the data samples associated with a group should appear all in the same fold. Please, find below a way to use the scikit-learn's GroupKFold object to create folds that meet this property (here, we simulate this scenario by considering only a 2-fold creation strategy):\n",
    "\n",
    "`X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "groups = np.array([0, 0, 2, 2])\n",
    "group_kfold = model_selection.GroupKFold(n_splits=2).split(X, y, groups)\n",
    "`\n",
    "\n",
    "Finally, we provide an illustrative example not related with the task on how this iterator can be then used to generate training and test folds:\n",
    "\n",
    "`\n",
    "for train_index, test_index in group_kfold:\n",
    "    print('TRAIN:', train_index, 'TEST:', test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(X_train, '-', X_test, '-', y_train, '-', y_test)\n",
    "`\n",
    "\n",
    "The above for loop generates the following output. It can be observed that the data samples belonging to a group all appear in the same fold, as expected. \n",
    "\n",
    "`TRAIN: [0 1] TEST: [2 3]\n",
    "[[1 2] [3 4]] - [[5 6] [7 8]] - [1 2] - [3 4] \n",
    "`\n",
    "\n",
    "`\n",
    "TRAIN: [2 3] TEST: [0 1]\n",
    "[[5 6] [7 8]] - [[1 2] [3 4]] - [3 4] - [1 2]\n",
    "`\n",
    "\n",
    "Please, find more information about the GroupKFold iterator in the [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "wc690YVPvKim",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a0d029bca463a8f1dd93f17bd7ad15c",
     "grade": true,
     "grade_id": "1-1",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_iterator(data):\n",
    "    '''\n",
    "    Create an iterator to split interactions in data in 10 folds, with the same student not appearing in multiple folds.\n",
    "    :param data:        Dataframe with student's interactions.\n",
    "    :return:            An initialized GroupKFold iterator.\n",
    "    '''\n",
    "    ### YOUR CODE HERE ###\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check outputs of this function and the properties of the iterator. Make sure that at each iteration: there no user_id in both train and test set(no overlap), all user_id in the dataset are contain in the train and test set (union), each user appears in the test set exactly once and other property you find usefull to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xzAgb7WgvKin"
   },
   "source": [
    "<a id=\"section1.2\"></a>\n",
    "### Task 1.2\n",
    "\n",
    "In this task, we ask you to evaluate a `BKT model` with all default parameters, namely `Model(seed=0)` in pyBKT, through a `user-stratified 10-fold cross-validation`, computing the following performance metrics: `RMSE` and `AUC`. To do so, you should use the `create_iterator` function, defined in Task 1.1, to create the training and test set for each fold, starting from the interactions in `data`. \n",
    "\n",
    "No plotting is needed, it is enough to print the scores for each metric in the cell.\n",
    "\n",
    "Please, note that this task may require a long running time (e.g., about 40 to 90 minutes), depending on your implementation and device. Just as an indication, on a Dell XPS 13, one fold lasts around 7 minutes.\n",
    "\n",
    "Look at the `BKT model` [documentation](https://github.com/CAHLR/pyBKT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "25wgCefYvKin",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8316306fa314de14b7165780fa3d8322",
     "grade": true,
     "grade_id": "1-2",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "cf17073b-278d-428d-b2a4-b0e763809144"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE (please, feel free to add extra cells to solve this task, after this first one) ###\n",
    "rmse_bkt, auc_bkt = [], []\n",
    "for iteration, (train_index, test_index) in enumerate(create_iterator(data)):\n",
    "    # Split data in training and test sets\n",
    "    X_train, X_test = ...\n",
    "    # Initialize the model\n",
    "    model = ...\n",
    "    # Fit the model\n",
    "    %time ...\n",
    "    # Compute RMSE (use the model evaluation methods)\n",
    "    train_rmse = ...\n",
    "    test_rmse = ...\n",
    "    rmse_bkt.append(test_rmse)\n",
    "    # Compute AUC (use the model evaluation methods)\n",
    "    train_auc = ...\n",
    "    test_auc = ...\n",
    "    auc_bkt.append(test_auc)\n",
    "    # Print progress\n",
    "    print('Iteration:', iteration, 'RMSE', (train_rmse, test_rmse), 'AUC', (train_auc, test_auc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean and standard deviation for both metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OeI5jhXCvKiq"
   },
   "source": [
    "<a id=\"section1.3\"></a>\n",
    "### Task 1.3\n",
    "\n",
    "In this task, we ask you to evaluate an `AFM model` with all default parameters (e.g., no custom bounds, default l2 regularization, and fit_intercept=True) through a `user-stratified 10-fold cross-validation`, computing the following performance metrics: `RMSE` and `AUC`. To do so, exactly as you should have done for the BKT model in Task 1.2, you should use the `create_iterator` function, defined in Task 1.1, to create the training and test set for each fold, starting from the interactions in `data`. \n",
    "\n",
    "No plotting is needed, it is enough to print the scores for each metric in the cell.\n",
    "\n",
    "The following cells include some utility functions that are needed to generate the `X` and `y` data in a format that is accepted by pyAFM model objects. To complete this task, you can build on top of the `X` and `y` created for you with the following cells. Please, refer to Tutorial 6 for further information on pyAFM. \n",
    "\n",
    "Take a look [here](https://github.com/cmaclell/pyAFM/blob/6150afdef7ab2eabff6c439accb5f9f81af34129/afms_workflow_predict.py#L11) to have ideas on how to use the `AFM model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Snneh2D8vKir"
   },
   "outputs": [],
   "source": [
    "def read_as_student_step(data):    \n",
    "    skills, opportunities, corrects, user_ids = [], [], [], []\n",
    "    \n",
    "    for row_id, (_, row) in enumerate(data.iterrows()):\n",
    "        \n",
    "        # Get attributes for the current interaction \n",
    "        user_id = row['user_id']\n",
    "        skill_name = row['skill_name']\n",
    "        correct = row['correct']\n",
    "        prior_success = row['prior_success']\n",
    "        prior_failure = row['prior_failure']\n",
    "        \n",
    "        # Update the number of opportunities this student had with this skill\n",
    "        opportunities.append({skill_name: prior_success + prior_failure})\n",
    "        \n",
    "        # Update information in the current \n",
    "        skills.append({skill_name: 1})\n",
    "\n",
    "        # Answer info\n",
    "        corrects.append(correct)\n",
    "        \n",
    "        # Student info\n",
    "        user_ids.append({user_id: 1})\n",
    "        \n",
    "    return (skills, opportunities, corrects, user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjk4wh0XvKir"
   },
   "outputs": [],
   "source": [
    "def prepare_data_afm(skills, opportunities, corrects, user_ids):\n",
    "\n",
    "    sv = feature_extraction.DictVectorizer()\n",
    "    qv = feature_extraction.DictVectorizer()\n",
    "    ov = feature_extraction.DictVectorizer()\n",
    "    S = sv.fit_transform(user_ids)\n",
    "    Q = qv.fit_transform(skills)\n",
    "    O = ov.fit_transform(opportunities)\n",
    "    X = sc.sparse.hstack((S, Q, O))\n",
    "    y = np.array(corrects)\n",
    "\n",
    "    return (X.toarray(), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLqHV0wLvKis"
   },
   "source": [
    "Prepare the X and y arrays to be used to evaluate the AFM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyHpdmyMvKis",
    "outputId": "2e13212d-18f2-41f7-9b54-104f80f77bbd"
   },
   "outputs": [],
   "source": [
    "%time skills, opportunities, corrects, user_ids = read_as_student_step(data)\n",
    "%time X, y = prepare_data_afm(skills, opportunities, corrects, user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "OJ803xnyvKit",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e56c17f22908fffa306759018b02bdde",
     "grade": true,
     "grade_id": "1-3",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "3723ff36-ccaf-4e5a-ca9a-19cd923404f5"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE (please, feel free to add extra cells to solve this task, after this first one) ###\n",
    "rmse_afm, auc_afm = [], []\n",
    "for iteration, (train_index, test_index) in enumerate(create_iterator(data)):\n",
    "    # Split data in training and test sets\n",
    "    X_train, X_test = ...\n",
    "    y_train, y_test = ...\n",
    "    # Initialize and fit the model\n",
    "    afm = ...\n",
    "    %time ...\n",
    "    # Make predictions \n",
    "    y_train_pred = ...\n",
    "    y_test_pred = ...\n",
    "    # Compute RMSE (use metrics package methods)\n",
    "    train_rmse = ...\n",
    "    test_rmse = ...\n",
    "    rmse_afm.append(test_rmse)\n",
    "    # Compute AUC (use metrics package methods)\n",
    "    train_auc = ...\n",
    "    test_auc = ...\n",
    "    auc_afm.append(test_auc)\n",
    "    # Print progress\n",
    "    print('Iteration:', iteration, 'RMSE', (train_rmse, test_rmse), 'AUC', (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHZWugs_vKit"
   },
   "source": [
    "<a id=\"section1.4\"></a>\n",
    "### Task 1.4\n",
    "\n",
    "In this task, we ask you to evaluate a `PFA model` with all default parameters (e.g., no custom bounds, default l2 regularization, and fit_intercept=True) through a `user-stratified 10-fold cross-validation`, computing the following performance metrics: `RMSE` and `AUC`. To do so, exactly as you should have done for the BKT and AFM models in Task 1.2 and 1.3, you should use the `create_iterator` function, defined in Task 1.1, to create the training and test set for each fold, starting from the interactions in `data`. \n",
    "\n",
    "No plotting is needed, it is enough to print the scores for each metric in the cell.\n",
    "\n",
    "The following cells include some utility functions that are needed to generate the `X` and `y` data in a format that is accepted by pyAFM model objects. To complete this task, you can build on top of the `X` and `y` created for you with the following cells. Please, refer to Tutorial 6 for further information on pyAFM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mp5Lnwi3vKit"
   },
   "outputs": [],
   "source": [
    "def read_as_success_failure(data):\n",
    "    n_succ, n_fail = [], []\n",
    "\n",
    "    # Create the n_succ and n_fail variables required by pyAFM\n",
    "    for i, row in data.iterrows():\n",
    "        n_succ.append({row['skill_name']: int(row['prior_success'])})\n",
    "        n_fail.append({row['skill_name']: int(row['prior_failure'])})\n",
    "        \n",
    "    return n_succ, n_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nnZY6IIvKiu"
   },
   "outputs": [],
   "source": [
    "def prepare_data_pfa(skills, corrects, user_ids, n_succ, n_fail):\n",
    "    \n",
    "    s = feature_extraction.DictVectorizer()\n",
    "    q = feature_extraction.DictVectorizer()\n",
    "    succ = feature_extraction.DictVectorizer()\n",
    "    fail = feature_extraction.DictVectorizer()\n",
    "    S = s.fit_transform(user_ids)\n",
    "    Q = q.fit_transform(skills)\n",
    "    succ = succ.fit_transform(n_succ)\n",
    "    fail = fail.fit_transform(n_fail)\n",
    "    X = sc.sparse.hstack((S, Q, succ, fail))\n",
    "    y = np.array(corrects)\n",
    "\n",
    "    return (X.toarray(), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqDObHVuvKiu"
   },
   "source": [
    "Prepare the X and y arrays to be used to evaluate the PFA model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ou_lrN-vKiu",
    "outputId": "c571f101-3ecd-445d-a5e8-04296046c84f"
   },
   "outputs": [],
   "source": [
    "%time n_succ, n_fail = read_as_success_failure(data)\n",
    "%time X, y = prepare_data_pfa(skills, corrects, user_ids, n_succ, n_fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "tetdb6JMvKiv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af79f22564baca57f2b52305a8208b6c",
     "grade": true,
     "grade_id": "1-4",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "c084dccd-8fb3-40d7-ee02-a201d0024975"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE (please, feel free to add extra cells to solve this task, after this first one) ###\n",
    "rmse_afm, auc_afm = [], []\n",
    "for iteration, (train_index, test_index) in enumerate(create_iterator(data)):\n",
    "    # Split data in training and test sets\n",
    "    X_train, X_test = ...\n",
    "    y_train, y_test = ...\n",
    "    # Initialize and fit the model\n",
    "    afm = ...\n",
    "    %time ...\n",
    "    # Make predictions \n",
    "    y_train_pred = ...\n",
    "    y_test_pred = ...\n",
    "    # Compute RMSE (use metrics package methods)\n",
    "    train_rmse = ...\n",
    "    test_rmse = ...\n",
    "    rmse_afm.append(test_rmse)\n",
    "    # Compute AUC (use metrics package methods)\n",
    "    train_auc = ...\n",
    "    test_auc = ...\n",
    "    auc_afm.append(test_auc)\n",
    "    # Print progress\n",
    "    print('Iteration:', iteration, 'RMSE', (train_rmse, test_rmse), 'AUC', (train_auc, test_auc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "o91dKRW-vKiv"
   },
   "source": [
    "<a id=\"section1.5\"></a>\n",
    "### Task 1.5\n",
    "\n",
    "In this task, we ask you to visually report the RMSE and AUC scores achieved by the three student's models in the user-stratified 10-fold cross validation performed in Task 1.2, 1.3, and 1.4 respectively, in such a way that the models' performances can be easily and appropriately compared against each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "i0Ro413avKiw",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7f828411bec2f92ae88c58458fb1da9",
     "grade": true,
     "grade_id": "1-5",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "7f45aae0-09f8-4634-d2bd-b27798936f8c"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE (please, feel free to add extra cells to solve this task, after this first one) ###\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMl_9cfCvKiw"
   },
   "source": [
    "### Task 1.6 Please compare and discuss the performance metric scores achieved by the student's models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "207a5583fd1899acf60f57e6a82ea18a",
     "grade": true,
     "grade_id": "1-6",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8b79fvFvKix"
   },
   "source": [
    "<a id=\"section1\"></a>\n",
    "## 2  Knowledge Tracing: Learning Curves Comparison \n",
    "----\n",
    "\n",
    "In this section, you should fit a Bayesian Knowledge Tracing (BKT) model on the three skills included in the `data` data set, and compute the corresponding predictions. Then, for each skill included in the data dataframe, you should visually report and discuss (i) the learning curve and (ii) the bar plot representing the number of students who reached a given number of opportunities for that skill, obtained through the BKT model fitted on that skill, in such a way that models' learning patterns can be easily and appropriately compared. No comparison with other baseline model is required.\n",
    "\n",
    "For your convenience, you will be guided in completing this section through three main tasks:\n",
    "- Task 2.1: BKT fit and prediction. \n",
    "- Task 2.2: Learning curves and bar plots generation.\n",
    "- Task 2.3: Learning curves and bar plots discussion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AjCAtH1vKix"
   },
   "source": [
    "<a id=\"section2.1\"></a>\n",
    "### Task 2.1\n",
    "\n",
    "In this task, we ask you to fit a BKT model with all default parameters, i.e., `Model(seed=0)` in pyBKT, on the full `data` data set (no split into train and test set needed as we are not assessing predictive performance of the model here). Once your BKT model is fitted, we ask you to appropriately create a dataframe named `predictions` with four columns `user_id`, `skill_name`, `y_true`, `y_pred_bkt`. This dataframe should include one row per interaction in `data`, where user_id is the id of the student associated with that interaction, skill_name is the name of the skill involved in that interaction, y_true is the true student's performance on that interaction (1 if correct at the first attempt, 0 otherwise), and y_pred_bkt is the prediction made by the pre-trained BKT model for that interaction.  \n",
    "\n",
    "Please, note that this task may require a long running time (e.g., about 10 to 20 minutes), depending on your implementation and device. Just as an indication, on a Dell XPS 13, the fit process lasts around 7 minutes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "1A6BtleZvKix",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9dd81314da6839b757e6415bf9bdb75a",
     "grade": true,
     "grade_id": "2-1",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "a69b531e-f233-492a-e4ed-fe6f656957a9"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE (please, feel free to add extra cells to solve this task, after this first one) ###\n",
    "# Initialize the model\n",
    "model = ...\n",
    "\n",
    "# Fit the model on the entire dataset\n",
    "%time ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "# Make predictions\n",
    "predictions = ...\n",
    "\n",
    "# Rename the dataframe columns as per instructions\n",
    "predictions.columns = ['user_id', 'skill_name', 'y_true', 'y_pred_bkt']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first ten rows as a double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiqsymkBvKiy"
   },
   "source": [
    "<a id=\"section2.4\"></a>\n",
    "### Task 2.2\n",
    "\n",
    "In this task, for each skill, we ask you to visually report and discuss (i) the `learning curve` and (ii) the `bar plot` representing the number of students who reached a given number of opportunities (similar to the visualizations done in Tutorial 6),  obtained by the BKT model fitted on that skill, in such a way that models' learning patterns can be easily and appropriately compared. To do so, we ask you to use the predictions you stored in the dataframe `predictions`.    \n",
    "\n",
    "No comparison with other baseline model is required.\n",
    "\n",
    "Please, refer to Tutorial 6 for further information on learning curve and bar plotting for student's knowledge tracing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "0hQVhP85vKiz",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfd497cead44c85762fd78dcf2a3c5ff",
     "grade": true,
     "grade_id": "2-2",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE (please, feel free to add extra cells to solve this task, after this first one) ###\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qr0RUHiqvKi0"
   },
   "source": [
    "### Task 2.3 Please discuss all visualizations (learning curves and bar plots) obtained with the BKT model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63c78cb2b4e8cecd2ee41801648e516c",
     "grade": true,
     "grade_id": "2-3",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework05-KnowledgeTracing_row_sol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
