{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8135284-1238-480e-89d8-5790b142ea66",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lecture 7 - Student Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494fe36e-c815-4bae-88b4-6c784e829d1d",
   "metadata": {},
   "source": [
    "In this exercises, you will create and interpret learning curves and compare the performance of different knowledge tracing models. We will use the same ASSISTments data set as for lecture 6.\n",
    "\n",
    "The ASSISTments data sets are often used for benchmarking knowledge tracing models. We will play with a simplified data set that contains the following columns:\n",
    "\n",
    "| Name                   | Description                         |\n",
    "| ---------------------- | ------------------------------------------------------------ |\n",
    "| user_id | The ID of the student who is solving the problem.  | |\n",
    "| order_id | The temporal ID (timestamp) associated with the student's answer to the problem.  | |\n",
    "| skill_name | The name of the skill associated with the problem. | |\n",
    "| correct | The student's performance on the problem: 1 if the problem's answer is correct at the first attempt, 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f92b49-a6b7-481e-abe3-8bd1d929965f",
   "metadata": {},
   "source": [
    "We first load the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3cc5d-dfdc-4214-b01c-0f9b8dfa77a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Principal package imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "# Scikit-learn package imports\n",
    "from sklearn import feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "\n",
    "# PyBKT package imports\n",
    "from pyBKT.models import Model\n",
    "# Import the lmm model class\n",
    "from pymer4.models import Lmer\n",
    "\n",
    "DATA_DIR = \"./../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e03b6b-462f-4770-af1c-c0b4fd00f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistments = pd.read_csv(DATA_DIR + 'assistments.csv', low_memory=False).dropna()\n",
    "assistments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462477a1-47f4-4610-b41e-569cf320903f",
   "metadata": {},
   "source": [
    "Next, we print the number of unique students and skills in this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8bfc64-ddd6-48b7-9a05-5a5453e5f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique students in the dataset:\", len(set(assistments['user_id'])))\n",
    "print(\"Number of unique skills in the dataset:\", len(set(assistments['skill_name'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b13c9b-53d6-4a23-9496-5d41798d1dc7",
   "metadata": {},
   "source": [
    "We also implement a utility function that splits the data in two folds, making sure that all interactions of a student land in the same fold. We will use this function to obtain train, test, and validation folds of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea098b-23ab-467f-981d-80b28f97dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iterator(data):\n",
    "    '''\n",
    "    Create an iterator to split interactions in data into train and test, with the same student not appearing in two diverse folds.\n",
    "    :param data:        Dataframe with student's interactions.\n",
    "    :return:            An iterator.\n",
    "    '''    \n",
    "    # Both passing a matrix with the raw data or just an array of indexes works\n",
    "    X = np.arange(len(data.index)) \n",
    "    # Groups of interactions are identified by the user id (we do not want the same user appearing in two folds)\n",
    "    groups = data['user_id'].values \n",
    "    return model_selection.GroupShuffleSplit(n_splits=1, train_size=.8, test_size=0.2, random_state=0).split(X, groups=groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9568390-1a42-48d8-b2b9-ac3a31333d34",
   "metadata": {},
   "source": [
    "## BKT Models - Learning Curves\n",
    "Last week, we have seen how to use BKT to predict the probability that a student will solve a task correctly.\n",
    "In addition, we can also use this type of model to compute learning curves and in this way analyze the learning activity (in our case the skills). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da430b-6fed-4f9c-a892-c70cb87bfda3",
   "metadata": {},
   "source": [
    "We first fit a BKT model with all default parameters, i.e., Model(seed=0) in pyBKT, on the full data data set (no split into train and test set needed as we are not assessing predictive performance of the model here, but just checking interpretation). To keep things simpler on the following 6 skills for this exercise:  \n",
    "`'Circle Graph', 'Venn Diagram', 'Mode', 'Division Fractions', 'Finding Percents', 'Area Rectangle'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256747ea-d957-4073-913d-cf2cfe383aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_subset = ['Circle Graph', 'Venn Diagram', 'Mode', 'Division Fractions', 'Finding Percents', 'Area Rectangle']\n",
    "\n",
    "data = assistments[assistments['skill_name'].isin(skills_subset)]\n",
    "\n",
    "print(\"Skill set:\", set(data['skill_name']))\n",
    "print(\"Number of unique students in the subset:\", len(set(data['user_id'])))\n",
    "print(\"Number of unique skills in the subset:\", len(set(data['skill_name'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826635fd-9716-4467-b78c-bf3916c087a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Model(seed=0)\n",
    "\n",
    "# Fit the model on the entire dataset\n",
    "%time model.fit(data=data)\n",
    "\n",
    "predictions = model.predict(data=data)[['user_id', 'skill_name', 'correct', 'correct_predictions']]\n",
    "\n",
    "# Rename the dataframe columns as per instructions\n",
    "predictions.columns = ['user_id', 'skill_name', 'y_true', 'y_pred_bkt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35931f35-9827-4274-b37e-ed2fc57635c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fd579-00f3-4dee-8b4c-dd8ee3562974",
   "metadata": {},
   "source": [
    "Next, we create a function that computes the learning curve (observed or predicted) for us by averaging over the success rate of all users at a given opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4ee28-bce8-4d57-8ab1-b10b6b5eec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_y_by_x(x, y):\n",
    "    '''\n",
    "    Compute average learning curve and number of students over the number of opportunities. \n",
    "    x is the number of opportunities.\n",
    "    y the success rates of the users (can be predicted success rate or true success rate).\n",
    "    '''\n",
    "    # Transform lists into arrays\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Sort the integer id representing the number of opportunities in increasing order\n",
    "    xs = sorted(list(set(x)))\n",
    "\n",
    "    # Supporting lists to store the:\n",
    "    # - xv: integer identifier of the number of opportunities\n",
    "    # - yv: average value across students at that number of opportunities\n",
    "    # - lcb and ucb: lower and upper confidence bound\n",
    "    # - n_obs: number of observartions present at that number of opportunities (on per-skill plots, it is the #students)\n",
    "    xv, yv, lcb, ucb, n_obs = [], [], [], [], []\n",
    "\n",
    "    # For each integer identifier of the number of opportunities 0, ...\n",
    "    for v in xs:\n",
    "        ys = [y[i] for i, e in enumerate(x) if e == v] # We retrieve the values for that integer identifier\n",
    "        if len(ys) > 0: \n",
    "            xv.append(v) # Append the integer identifier of the number of opportunities\n",
    "            yv.append(sum(ys) / len(ys)) # Append the average value across students at that number of opportunities\n",
    "            n_obs.append(len(ys)) # Append the number of observartions present at that number of opportunities\n",
    "\n",
    "            \n",
    "            # Prepare data for confidence interval computation\n",
    "            unique, counts = np.unique(ys, return_counts=True)\n",
    "            counts = dict(zip(unique, counts))\n",
    "\n",
    "            if 0 not in counts:\n",
    "                counts[0] = 0\n",
    "            if 1 not in counts:\n",
    "                counts[1] = 0\n",
    "\n",
    "            # Calculate the 95% confidence intervals\n",
    "            ci = sc.stats.beta.interval(0.95, 0.5 + counts[0], 0.5 + counts[1])\n",
    "            lcb.append(ci[0])\n",
    "            ucb.append(ci[1])\n",
    "\n",
    "    return xv, yv, lcb, ucb, n_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7d82c0-38ce-4b0d-8352-703d6762cb4f",
   "metadata": {},
   "source": [
    "Then, we create a function for plotting learning curve and a bar chart with the number of students per opportunity for a given skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2d0ab-3221-4bf4-b7e6-2f8b124a72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(skill_name):\n",
    "    '''\n",
    "    Plot learning curve using BKT model for skill `skill_name`. \n",
    "    '''\n",
    "    preds = predictions[predictions['skill_name'] == skill_name] # Retrieve predictions for the current skill\n",
    "\n",
    "    xp = []\n",
    "    yp = {}\n",
    "    for col in preds.columns: # For y_true and and y_pred_bkt columns, initialize an empty list for curve values\n",
    "        if 'y_' in col:\n",
    "            yp[col] = []\n",
    "\n",
    "    for user_id in preds['user_id'].unique(): # For each user\n",
    "        user_preds = preds[preds['user_id'] == user_id] # Retrieve the predictions on the current skill for this user \n",
    "        xp += list(np.arange(len(user_preds))) # The x-axis values go from 0 to |n_opportunities|-1\n",
    "        for col in preds.columns: \n",
    "            if 'y_' in col: # For y_true and and y_pred_bkt columns\n",
    "                yp[col] += user_preds[col].tolist() # The y-axis value is the success rate for this user at that opportunity\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 2]}) # Initialize the plotting figure\n",
    "\n",
    "    lines = []\n",
    "    for col in preds.columns:\n",
    "        if 'y_' in col: # For y_true and and y_pred_bkt columns   \n",
    "            x, y, lcb, ucb, n_obs = avg_y_by_x(xp, yp[col]) # Calculate mean and 95% confidence intervals for success rate \n",
    "            y = [1-v for v in y] # Transform success rate in error rate\n",
    "            if col == 'y_true': # In case of ground-truth data, we also show the confidence intervals\n",
    "                axs[0].fill_between(x, lcb, ucb, alpha=.1)\n",
    "            model_line, = axs[0].plot(x, y, label=col) # Plot the curve\n",
    "            lines.append(model_line) # Store the line to then set the legend    \n",
    "\n",
    "    # Make decorations for the learning curve plot\n",
    "    axs[0].set_title(skill_name)\n",
    "    axs[0].legend(handles=lines)\n",
    "    axs[0].set_ylabel('Error')\n",
    "    axs[0].set_ylim(0, 1)\n",
    "    axs[0].set_xlim(0, None)\n",
    "\n",
    "    # Plot the number of observations per number of opportunities bars and make decorations\n",
    "    axs[1].set_xlabel('#Opportunities')\n",
    "    axs[1].bar([i for i in range(len(n_obs))], n_obs)\n",
    "    axs[1].set_ylabel('#Observations')\n",
    "    axs[1].set_ylim(0, 750)\n",
    "    axs[1].set_xlim(0, None)\n",
    "\n",
    "    # Plot the learning curve and the bar plot \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205fcf6-3dcf-4f5c-be70-fe88a93a0a7a",
   "metadata": {},
   "source": [
    "We then plot the learning curve and number of opportunities per student for skill `Circle Graph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3b0499-c5df-4c1e-8c5d-8a8310cb5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = plot_learning_curve('Circle Graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7406f4b-34a6-4b67-9a6d-cde06315890d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Your Turn 1 - Learning Curves\n",
    "\n",
    "Visualize and interpret the learning curves and number of students per opportunity for two selected skills. You can choose from the remaining five skills: `'Venn Diagram', 'Mode', 'Division Fractions', 'Finding Percents', 'Area Rectangle'`. Send us your visualizations as well as the discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe40f4-a87c-4e3c-9d17-374237026a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "exec(requests.get(\"https://courdier.pythonanywhere.com/get-send-code\").content)\n",
    "\n",
    "npt_config = {\n",
    "    'session_name': 'lecture-07',\n",
    "    'session_owner': 'mlbd',\n",
    "    'sender_name': input(\"Your name: \"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd3759e-f5d4-4278-aa0f-613a56a3673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Visualize the learning curve for the first skill.\n",
    "\n",
    "first_skill_name = \"1\" # replace the skill name with one of the 5 skills above\n",
    "plt = plot_learning_curve(first_skill_name)\n",
    "\n",
    "### Share the plot with us\n",
    "send(plt, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307e90b-00fe-4c09-878d-2a87aeaae7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: What is your analysis about the learning curve for the first skill?\n",
    "\n",
    "### Share your analysis of the learning curve with us\n",
    "first_skill_interpretation = \"Write your interpretation here\"\n",
    "send(first_skill_interpretation, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271b060-84cb-40cc-8492-cad837129738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Visualize the learning curve for the second skill.\n",
    "\n",
    "second_skill_name = \"2\" # replace the skill name with one of the 5 skills above\n",
    "plt = plot_learning_curve(second_skill_name)\n",
    "\n",
    "### Share the plot with us\n",
    "send(plt, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f0e2ed-69f9-4ca7-8ba9-6355faf0c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: What is your analysis about the learning curve for the second skill?\n",
    "\n",
    "### Share your analysis of the learning curve with us\n",
    "second_skill_interpretation = \"Write your interpretation here\"\n",
    "send(second_skill_interpretation, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca629369-e82e-4f59-a086-d07c3e63351c",
   "metadata": {},
   "source": [
    "## Additive Factors Model (AFM) and Performance Factors Analysis (PFA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8340908f-22c1-4e1f-a631-ff12be331ec0",
   "metadata": {
    "tags": []
   },
   "source": [
    "The AFM and PFA models are both based on logistic regression and item response theory (IRT). Specifically, they compute the probability that a student will solve a task correctly based on the number of previous attempts the student had at the corresponding skill (in case of AFM) and based on the correct and wrong attempts at the corresponding skill (in case of PFA), respectively. We therefore first preprocess the data to compute these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a25b8d-1acb-4d7c-bc44-45319a193a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "# Number of attempts before current\n",
    "def preprocess_data(data):\n",
    "    data.loc[:, 'aux'] = 1\n",
    "    data.loc[:, 'prev_attempts'] = data.sort_values('order_id').groupby(['user_id', 'skill_name'])['aux'].cumsum() -1\n",
    "\n",
    "    # Number of correct and incorrect attempts before current attempt\n",
    "    data.loc[:, 'correct_aux'] = data.sort_values('order_id').groupby(['user_id', 'skill_name'])['correct'].cumsum()\n",
    "    data.loc[:, 'before_correct_num'] = data.sort_values('order_id').groupby(['user_id', 'skill_name'])['correct_aux'].shift(periods=1, fill_value=0)\n",
    "    data.loc[:, 'before_wrong_num'] = data['prev_attempts'] - data['before_correct_num']\n",
    "    return data\n",
    "\n",
    "data = preprocess_data(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ebc58-9d11-4aae-bd8c-bd514c8d8ded",
   "metadata": {},
   "source": [
    "Next, we split the data into a training and a test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c1ff6-e1c0-406a-ada9-c9408a88a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain indexes\n",
    "train_index, test_index = next(create_iterator(data))\n",
    "# Split the data\n",
    "X_train, X_test = data.iloc[train_index], data.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c9fe3-b53b-4d4e-ab63-42ed014062c2",
   "metadata": {},
   "source": [
    "Next, we fit an AFM model to the training data and predict on the test data. Note that the implementation below only works for a one-to-one correspondance of task and skill, i.e. when a task is associated to exactly one skill. In case of a data set containing tasks with multiple skills, we would need to use the [pyAFM](https://github.com/cmaclell/pyAFM) package. A tutorial on using pyAFM can be found [here](https://github.com/epfl-ml4ed/mlbd-2021/tree/main/Tutorials/Tutorial06/Tutorial06)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc29981-4a6d-4678-9a4c-f9dca55432c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the model\n",
    "model = Lmer(\"correct ~ (1|user_id) + (1|skill_name) + (0 + prev_attempts|skill_name)\", data=X_train, family='binomial')\n",
    "%time model.fit() \n",
    "# Compute predictions\n",
    "X_test['afm_predictions'] = model.predict(data=X_test, verify_predictions=False)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857fc582-3635-4177-9570-d6ac55af27df",
   "metadata": {},
   "source": [
    "Next, we fit a PFA model to the data. Again, this implementation works for one-to-one correspondance and tasks with multiple skills would require the use of [pyAFM](https://github.com/cmaclell/pyAFM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b111e-0433-4abe-a717-34149538b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the model\n",
    "model = Lmer(\"correct ~ (1|user_id) + (1|skill_name) + (0 + before_correct_num|skill_name) + (0 + before_wrong_num|skill_name)\", data=X_train, family='binomial')\n",
    "%time model.fit()\n",
    "# Compute predictions\n",
    "X_test['pfa_predictions'] = model.predict(data=X_test, verify_predictions=False)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea8507-82fc-4a0a-afd8-cf0f3b88f409",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BKT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1c717f-5cab-48c6-b4a3-495154b5cf24",
   "metadata": {},
   "source": [
    "We first also fit a BKT model to this data set using the same train/test split as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044fa2b-0958-41ff-92cc-cc5a7bc4c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame()\n",
    "\n",
    "# Train a BKT model for each skill\n",
    "for skill in skills_subset:\n",
    "    print(\"--{}--\".format(skill))\n",
    "    X_train_skill = X_train[X_train['skill_name'] == skill]\n",
    "    X_test_skill = X_test[X_test['skill_name'] == skill]\n",
    "    # Initialize and fit the model\n",
    "    model = Model(seed=0)\n",
    "    %time model.fit(data=X_train_skill) \n",
    "    preds = model.predict(data=X_test_skill) [['user_id', 'order_id', 'skill_name', 'correct', 'prev_attempts',\n",
    "       'before_correct_num', 'before_wrong_num', 'afm_predictions', 'pfa_predictions', 'correct_predictions']]\n",
    "    df_preds = df_preds.append(preds)\n",
    "\n",
    "X_test = df_preds\n",
    "X_test.columns = ['user_id', 'order_id', 'skill_name', 'correct', 'prev_attempts',\n",
    "       'before_correct_num', 'before_wrong_num', 'afm_predictions', 'pfa_predictions', 'bkt_predictions']\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a1b20-ff3b-4c43-8365-59e7c9cb4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('x_test_07.csv.gz', compression = 'gzip', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d43b8-0993-4a73-9cfe-7038ca4c165a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Your Turn 2 - Model Comparison on Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49357c01-fcaa-4287-ab57-3a62bb9a5aed",
   "metadata": {},
   "source": [
    "Up to now, we have compared model performance on a subset of the data. Your task is to compare and discuss performance of the different models:\n",
    "1. Visualize the overall RMSE and AUC of the four models (AFM, PFA, BKT) such that the metrics can be easily compared.\n",
    "2. Interpret your results and discuss your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e314a549-c21c-4ed5-8be0-672aa147b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it is taking too long to run, you may load our X_test to compute the RMSE and AUC\n",
    "X_test = pd.read_csv('x_test_07.csv.gz', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe56c0e-3441-4062-bc3f-4a3da03d03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize plots\n",
    "\n",
    "send(plt, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55380e4c-0241-4516-8ae7-dd934abb20c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation = \"Write your interpretation here\"\n",
    "\n",
    "send(interpretation, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
