{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "formed-footwear",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline \n",
    "\n",
    "This notebook is meant to be a brief and simple introduction to pipelines with the hope that it will spark your interest to learn more.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-cookie",
   "metadata": {},
   "source": [
    "### Why should you create a pipeline?\n",
    "* Reusable across projects\n",
    "* Test new ideas (components easily)\n",
    "* Reduce bugs/erros\n",
    "* Prevents data leaking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "starting-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, \\\n",
    "RobustScaler, MaxAbsScaler\n",
    "from sklearn import set_config\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-imaging",
   "metadata": {},
   "source": [
    "## 0 - Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stylish-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[50:]  # we only take the first two classes\n",
    "y = iris.target[50:] # binary classification\n",
    "\n",
    "# Split the data into train and test (val)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, \n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-money",
   "metadata": {},
   "source": [
    "## 1 - Simple pipeline\n",
    "We will explore sklearn [pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline) class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "polar-atmosphere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.95      0.95      0.95        20\n",
      "weighted avg       0.95      0.95      0.95        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Without the pipeline\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-spiritual",
   "metadata": {},
   "source": [
    "How does this scale when we have more steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tamil-manor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.95      0.95      0.95        20\n",
      "weighted avg       0.95      0.95      0.95        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With the pipeline\n",
    "steps = [('scaler', StandardScaler()), # preprocessing steps\n",
    "         ('SVM', SVC())]               # model\n",
    "\n",
    "pipeline = Pipeline(steps) \n",
    "pipeline.fit(X_train, y_train) \n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-helen",
   "metadata": {},
   "source": [
    "## 2 - Pipeline with cross-validation\n",
    "\n",
    "Without the pipeline, if you want to prevent data leaking, you need to standardize separetly on every fold! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cordless-postcard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.90909091, 1.        , 0.90909091, 0.88888889,\n",
       "       0.88888889, 0.8       , 1.        , 1.        , 1.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('scaler', StandardScaler()), # preprocessing steps\n",
    "         ('SVM', SVC())]               # model\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# We send X and y complete\n",
    "cross_val_score(pipeline, X, y, cv=10, scoring =\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-correlation",
   "metadata": {},
   "source": [
    "## 3 - Pipeline with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "equal-barrier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.932):\n",
      "{'SVM__C': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    'SVM__C': np.logspace(-4, 4, 4),\n",
    "}\n",
    "\n",
    "steps = [('scaler', StandardScaler()), # preprocessing steps\n",
    "         ('SVM', SVC())]               # model\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, n_jobs=-1, cv = 10, scoring = \"f1\")\n",
    "search.fit(X, y)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-agriculture",
   "metadata": {},
   "source": [
    "## 4 - Pipeline with hyper-hyper parameter GridSearch\n",
    "What if you want to explore different preprocessing steps (scalers)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "655bacd2-cbb6-4019-8296-5c6dbf0255d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler: StandardScaler()\n",
      "Best parameter (CV score=0.940):\n",
      "{'clf__C': 0.0001, 'scaler': StandardScaler()}\n"
     ]
    }
   ],
   "source": [
    "scalers =  [\n",
    "    StandardScaler(),\n",
    "    RobustScaler(),\n",
    "    'passthrough'] # none\n",
    "\n",
    "\n",
    "steps = [('scaler', StandardScaler()), # preprocessing steps\n",
    "         ('clf', SVC())]               # Model\n",
    "\n",
    "param_grid = {\n",
    "    'scaler': scalers,\n",
    "    'clf__C': np.logspace(-4, 4, 4),\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, n_jobs=-1, cv = 10, scoring = \"recall\")\n",
    "search.fit(X, y)\n",
    "print(\"Scaler: %s\" % scaler)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-eagle",
   "metadata": {},
   "source": [
    "**What if we want to explore different models?**\n",
    "\n",
    "Note: here we could do it in different steps so that we don't scale the data multiple times.\n",
    "We could scale once and then test different classifiers.\n",
    "We decided to use the complete pipeline for code consistency. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3f78aae-d0cd-4812-8809-8741e4c3f937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler: StandardScaler()\n",
      "Best parameter (CV score=0.960):\n",
      "{'clf': LogisticRegression(C=21.54434690031882), 'clf__C': 21.54434690031882, 'scaler': StandardScaler()}\n"
     ]
    }
   ],
   "source": [
    "scalers =  [\n",
    "    StandardScaler(),\n",
    "    RobustScaler(),\n",
    "    'passthrough'] # none\n",
    "\n",
    "classifiers = [\n",
    "    SVC(),\n",
    "    LogisticRegression()\n",
    "]\n",
    "\n",
    "steps = [('scaler', StandardScaler()), # preprocessing steps\n",
    "         ('clf', SVC())]               # Model\n",
    "\n",
    "param_grid = {\n",
    "    'scaler': scalers,\n",
    "    'clf': classifiers,\n",
    "    'clf__C': np.logspace(-4, 4, 4),\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, n_jobs=-1, cv = 10, scoring = \"recall\")\n",
    "search.fit(X, y)\n",
    "print(\"Scaler: %s\" % scaler)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-scenario",
   "metadata": {},
   "source": [
    "What other things can you play with? \n",
    "\n",
    "- Preprocessing data\n",
    "    - Standardization, or mean removal and variance scaling\n",
    "    - Non-linear transformation\n",
    "    - Normalization\n",
    "    - Encoding categorical features\n",
    "    - Discretization\n",
    "    - Imputation of missing values\n",
    "    - Generating polynomial features\n",
    "- Imputation of missing values\n",
    "    - Univariate vs. Multivariate Imputation\n",
    "    - Univariate feature imputation\n",
    "    - Multivariate feature imputation\n",
    "    - Nearest neighbors imputation\n",
    "    - Marking imputed values\n",
    "- Feature selection\n",
    "- Dimensionality reduction\n",
    "- Modeling\n",
    "\n",
    "More ideas [here](https://scikit-learn.org/stable/data_transforms.html)\n",
    "\n",
    "**What if I can't find the one I need?**\n",
    "[Create it](https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156)! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86403fd7-a6f9-46c5-9dfc-f84ad7afb2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
